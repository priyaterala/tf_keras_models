import tensorflow as tf
from tensorflow import keras

import numpy as np

print(tf.__version__)                                         ======>1.9.0
#############################################

boston_housing = keras.datasets.boston_housing
(train_data, train_labels), (test_data, test_labels) = boston_housing.load_data()

# Shuffle the training set
order = np.argsort(np.random.random(train_labels.shape))
train_data = train_data[order]
train_labels = train_labels[order]

print("Training set: {}".format(train_data.shape))  
print("Testing set:  {}".format(test_data.shape))  

print(train_data[0])
#############################################
======>Training set: (404, 13)
	Testing set:  (102, 13)
	[7.8750e-02 4.5000e+01 3.4400e+00 0.0000e+00 4.3700e-01 6.7820e+00
	 4.1100e+01 3.7886e+00 5.0000e+00 3.9800e+02 1.5200e+01 3.9387e+02
 	6.6800e+00]
#############################################
import pandas as pd

column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',
                'TAX', 'PTRATIO', 'B', 'LSTAT']

df = pd.DataFrame(train_data, columns=column_names)
df.head()
#############################################

======>
	CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT
0	0.07875	45.0	3.44	0.0	0.437	6.782	41.1	3.7886	5.0	398.0	15.2	393.87	6.68
1	4.55587	0.0	18.10	0.0	0.718	3.561	87.9	1.6132	24.0	666.0	20.2	354.70	7.12
2	0.09604	40.0	6.41	0.0	0.447	6.854	42.8	4.2673	4.0	254.0	17.6	396.90	2.98
3	0.01870	85.0	4.15	0.0	0.429	6.516	27.7	8.5353	4.0	351.0	17.9	392.43	6.36
4	0.52693	0.0	6.20	0.0	0.504	8.725	83.0	2.8944	8.0	307.0	17.4	382.00	4.63
#############################################

print(train_labels[0:10])  # Display first 10 entries
                       ======>[32.  27.5 32.  23.1 50.  20.6 22.6 36.2 21.8 19.5]
#############################################

# Test data is not used when calculating the mean and std.

mean = train_data.mean(axis=0)
std = train_data.std(axis=0)
train_data = (train_data - mean) / std
test_data = (test_data - mean) / std

print(train_data[0])  # First training sample, normalized
#############################################

======>[-0.39725269  1.41205707 -1.12664623 -0.25683275 -1.027385    0.72635358
	 -1.00016413  0.02383449 -0.51114231 -0.04753316 -1.49067405  0.41584124
 	-0.83648691]
#############################################

def build_model():
  model = keras.Sequential([
    keras.layers.Dense(64, activation=tf.nn.relu, 
                       input_shape=(train_data.shape[1],)),
    keras.layers.Dense(64, activation=tf.nn.relu),
    keras.layers.Dense(1)
  ])

  optimizer = tf.train.RMSPropOptimizer(0.001)

  model.compile(loss='mse',
                optimizer=optimizer,
                metrics=['mae'])
  return model

model = build_model()
model.summary()
#############################################
 
======>
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 64)                896       
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160      
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 5,121
Trainable params: 5,121
Non-trainable params: 0
_________________________
#############################################

# Display training progress by printing a single dot for each completed epoch.
class PrintDot(keras.callbacks.Callback):
  def on_epoch_end(self,epoch,logs):
    if epoch % 100 == 0: print('')
    print('.', end='')

EPOCHS = 500
# Store training stats
history = model.fit(train_data, train_labels, epochs=EPOCHS,
                    validation_split=0.2, verbose=0,
                    callbacks=[PrintDot()])
#############################################
=====>
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
....................................................................................................
#############################################

import matplotlib.pyplot as plt

def plot_history(history):
  plt.figure()
  plt.xlabel('Epoch')
  plt.ylabel('Mean Abs Error [1000$]')
  plt.plot(history.epoch, np.array(history.history['mean_absolute_error']), 
           label='Train Loss')
  plt.plot(history.epoch, np.array(history.history['val_mean_absolute_error']),
           label = 'Val loss')
  plt.legend()
  plt.ylim([0,5])

plot_history(history)
#############################################

[loss, mae] = model.evaluate(test_data, test_labels, verbose=0)
print("Testing set Mean Abs Error: ${:7.2f}".format(mae * 1000))
#############################################

=====>Testing set Mean Abs Error: $2531.75
#############################################

test_predictions = model.predict(test_data).flatten()

print(test_predictions)
#############################################

=====>
[ 7.921479  19.1692    21.631945  32.08326   26.638332  21.435698
 25.38551   22.235943  19.785725  22.56226   21.218086  17.125937
 17.03408   42.62962   19.060608  20.000294  26.077763  17.910877
 18.920856  25.687073  11.246421  12.643982  22.453918  15.175223
 19.459389  26.094055  29.516502  28.894056  10.356123  20.011765
 19.870594  14.292997  33.991295  24.544125  17.690891   7.517699
 17.91885   17.621078  19.958347  25.157063  28.978596  28.01129
 13.5498905 43.302456  30.226303  27.849524  27.973042  17.950552
 23.022419  22.430586  34.2196    19.72445    9.380225  14.089563
 35.71028   28.817255  12.260198  47.413868  35.393497  24.635174
 24.332912  15.287039  14.311161  19.956076  24.390125  20.850906
 13.802902  21.766884   9.041606   8.325125  24.401306  28.699444
 26.968954   8.578408  23.747272  18.156225  19.232525  23.824238
 35.728683   9.143802  22.666533  37.41384   15.907543  14.030156
 16.995594  18.598364  20.075703  19.693733  21.846174  33.559032
 20.76694   19.159264  25.18504   41.622425  37.08427   21.48124
 35.129402  42.5294    25.48027   45.9865    31.550156  20.011475 ]
#############################################
